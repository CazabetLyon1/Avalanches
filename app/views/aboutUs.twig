{% extends "layout.twig" %}

    {% block content %}

        <div id="apropos" class="container-fluid">

            <div class="row aboutUs">
                <h2>Les développeurs</h2>
                <div>
                    Ce site web a été développé par Gérome Ferrand et Aymeric Touche.<br>
                    Nous sommes deux étudiants de Licence 3 Informatique à l'Université Lyon I.
                </div>
            </div>
            <div class="row aboutUs">
                <h2>Le contexte</h2>
                <div>
                    Ce site a été réalisé dans le cadre de l'UE LIFPROJET.<br>
                    L'objectif de cette UE est de mener à bien un projet informatique en groupe de deux ou trois étudiants.
                </div>
            </div>
            <div class="row aboutUs">
                <h2>Le sujet</h2>
                <div>
                    Le principe de ce projet est de choisir une source de données accessible (open data, data crawling, etc), <br>de collecter ces données, les analyser, et construire un “explorable” permettant à n'importe qui d'explorer ces données de manière interactive. <br><br>
                    La thématique des avalanches a été très vite validée, l'un de nous étant passionné de montagne.<br>
                    Le support web a été choisi car nous voulions progresser dans cette matière<br><br>

                    Plusieurs sources de données ont été explorées et exploitées:<br><br>
                    Tout d'abord nous avons trouvé un premier jeu de données de <a href="https://www.data.gouv.fr/en/datasets/clpa-photo-interpretation-zones-des-phenomenes-davalanche-de-rhone-alpes/">Photo-interprétation</a> collecté par l'IRSTEA, nommé CLPA.<br><br>

                    Par la suite nous avons exploité un jeu de données montrant les accidents dûs aux avalanches trouvé sur le site de l'<a href="http://www.anena.org/5041-bilan-des-accidents.htm">l'ANENA</a>.<br><br>

                    Enfin nous avons utilisé un scraper afin de parcourir le site <a href="http://www.data-avalanche.org/">data-avalanche.org</a> et de récupérer des données sur les avalanches que l'association a recensées.<br><br>

                    Nous avons utilisé l'outil <a href="https://www.mapbox.com/">MAPBOX</a> afin de créer une carte interactive affichant les données.<br><br>
                </div>
            </div>
            <div class="row aboutUs">
                <h2>Nos objectifs</h2>
                <div>
                    <ul class="list-group">
                        <li>Collecter des données sur le net  <span class="badge badge-success">OK</span></li>
                        <li>Analyser les caractéristiques principales d’une avalanche  <span class="badge badge-success">OK</span></li>
                        <li>Cartographier les avalanches en fonction de ces caractéristiques  <span class="badge badge-success">OK</span></li>
                        <li>Etablir une analyse graphique via des statistiques  <span class="badge badge-success">OK</span></li>
                        <li>Faire ressortir des indicateurs et en dégager un sens  <span class="badge badge-danger">KO</span></li>
                        <li>Rester ouverts aux possibilités offertes par les jeux de données collectés  <span class="badge badge-success">OK</span></li>
                    </ul>
                </div>
            </div>

            <div class="row aboutUs">
                <h2>Les technologies</h2>
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/Slim-Php.png">
                    </div>
                    <div class="descriptifTechno">
                        SLIM est un micro Framework PHP très léger. Il est idéal pour créer des applications web et des API.<br>
                        Etant plus léger il laisse plus de libertés que de gros framework tels que Symfony ou Laravel du fait qu'il contient moins de fonctionnalités.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/twig.png">
                    </div>
                    <div class="descriptifTechno">
                        Twig est un moteur de templates qui peut être intégré à Slim PHP. Il permet une gestion plus fine des différentes pages.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/htmlCSS.png">
                    </div>
                    <div class="descriptifTechno">
                        Ce sont les langages de bases de la programmation web, ils permettent de gérer les contenus d'une page web ainsi que son design de manière statique.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/bootstrap.png">
                    </div>
                    <div class="descriptifTechno">
                        Bootstrap est une bibliothèque CSS permettant de créer des modèles de page et de rendre cohérent le design d'un site plus facilement.
                    </div>
                </div>

                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/JS.png">
                    </div>
                    <div class="descriptifTechno">
                        JavaScript est un langage coté client qui permet de rendre des pages web dynamiques et interactives pour l'utilisateur.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/jQuery.svg">
                    </div>
                    <div class="descriptifTechno">
                        JQuery est une bibilothèque Javascript. Elle rend le code plus lisible et plus simple. <br>
                        Nous n'avions pas besoin de nous servir de JQuery en plus de JavaScript, mais nous voulions progresser dans ces deux langages.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/mapbox.png">
                    </div>
                    <div class="descriptifTechno">
                        MAPBOX GL JS est une autre bibliothèque JavaScript. Elle nous a été indispensable dans l'affichage de la carte et dans son utilisation.
                    </div>
                </div>
                
                <div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/python.png">
                    </div>
                    <div class="descriptifTechno">
                        Contrairement aux précédents, ce langage n'est pas prédestiné au web.<br>
						Il nous a servi à scrapper des données sur le Réseau, formater les données afin de garantir leur affichage sur le site et analyser le dataset issue du site data-avalanche.org.<br>
						Python est l'un des langages de prédilection dans le domaine de la manipulation et l'analyse de données, mais également lorsque l'on souhaite mettre en place des scripts légers et performants.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/scrapy.png">
                    </div>
                    <div class="descriptifTechno">
                        Scrapy est un outil de développement de robots d'indexation (plus communément appelés crawlers, web-spiders ou encore scrapers) en python.<br>
						Nous avons utilisé ce module dans le but de réunir des données à afficher sur la carte du site.<br>
						Or il s'est avéré que cette technologie n'était pas adaptée au site ayant le plus de données (data-avalanche.org) car les pages été gérées dynamiquement (i.e. en utilisant du JS).<br>
						Nous avons donc dû adopter une solution plus adaptée.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/selenium.png">
                    </div>
                    <div class="descriptifTechno">
                        Selenium, à l'instar de scrapy, est (entre autres) utilisé pour parcourir le web à la recherche d'informations. <br>
						Initialement, il s'agit d'un outil interagissant avec les navigateurs web dans le but de procéder à des tests dynamiques en simulant le comportement d'un utilisateur.<br>
						Selenium nous a donc permis de répondre à l'inéfficacité de scrapy à parcourir des sites dynamiques.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/numpy.png">
                    </div>
                    <div class="descriptifTechno">
                        Numpy est la bibilothèque python permettant entre autre la manipulation de tableau car python se base essentiellement sur un système de listes.<br>
						Suite à quelques essais avec ce module, nous avons davantage opté pour pandas/matplotlib.<br>
						Les tableaux proposés par Numpy ne permettent de gérer que des données de types similaires,<br>
						par ailleurs la conversion du fichier json vers ces tableaux est plus complexe qu'avec les structures de données de pandas. 
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/scipy.png">
                    </div>
                    <div class="descriptifTechno">
                        Scipy, qui fournit notamment des outils de statistiques (et plus généralement scientifiques), a été couplé dans un premier temps à Numpy afin de faire émerger du sens de ce dataset.<br>
						Scipy nous semblait particulièrement intéressant dans la mesure où il nous fournissait des données sous la forme de matrices creuses.<br>
						Cela nous aurait ainsi permis de gagner en fluidité lors de la manipulation d'importants volumes de données.<br>
						Néanmoins, nous avions sous-estimé la puissance de calcul de nos machines, car même sans cette technologie, le temps d'éxécution est tout à fait acceptable.<br>
						Or, une fois de plus, nous nous sommes réorienté vers les 2 bibliothèques suivantes.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/pandas.png">
                    </div>
                    <div class="descriptifTechno">
                        Pandas répondait parfaitement à nos besoins. En effet, cette bibliothèque python a été conçue pour la manipulation et l'annalyse de données.<br>
						Pandas se manifeste particulierement à travers ses structures de données : Les dataframes et leurs composants, les séries.<br>
						Toutes les structure contenant les données sur la page concernant l'analyse du dataset ont été permises par Pandas. 
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/matplotlib.png">
                    </div>
                    <div class="descriptifTechno">
                        MatPlotLib est le module python qui nous a permis de réprésenter les données sur des courbes, 'pie', histogrammes en barres, etc..<br>
						Inspirée de MatLab, il s'agit probablement de la lib la plus célèbre de python dans la représentation de données.<br>
						Toutes les figures présentes sur la page de l'analyse ont été réalisées avec MatPlotLib.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/Spyder.png">
                    </div>
                    <div class="descriptifTechno">
                        Spyder est l'IDE utilisé initialement pour la partie faite en python (essentiellement les scrapers).<br>
						Cependant, Sur les conseils de notre entourage, nous avons décidé de tenter par la suite la solution proposée par JetBrain : PyCharm.<br>
						Au terme des parties conçues avec les IDE python, Spyder s'est avéré plus simple d'utilisation et plus léger.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/PyCharm.png">
                    </div>
                    <div class="descriptifTechno">
                        PyCharm (community édition) est l'environnement de développement intégré développé par JetBrain.<br>
						Il offre un confort certain pour les projets de grande envergure, cepandant pour les scripts plus modestes, un IDE plus léger est plus adéquate.
                    </div>
                </div>
				<div class="techno">
                    <div class="logo">
                        <img class="imgLogo" src="public/img/jupyter.png">
                    </div>
                    <div class="descriptifTechno">
                        Jupyter notebook est une application web, et donc accesible via un navigateur, permettant d'interpréter du code python (parmi tant d'autres).<br>
						Il se compose d'une liste de cellule au sein desquelles on peut exécuter du code mais également incorporer des cellules dites markdown permettant le traitement et la mise en forme de texte.<br>
						Enfin, le fichier final peut être exporté en HTML, comme vous pouvez le constater sur la page de l'annalyse du dataset.<br>
						En effet, c'est une technologie parfaite lorsque l'on souhaite exposer du code, faire une démonstration, etc...
                    </div>
                </div>
            </div>  
            <div class="row aboutUs">
                <h2>La carte interactive</h2>
                <div>
                    Le site MapBox.com permet de créer un fond de carte que nous avons adapté au sujet, avec les reliefs. La carte interactive a été créée grâce à la bibliothèque MAPBOX GL JS qui est une librairie JavaScript.<br>
                    Cet outil permet de créer différents layers traduisant les fichiers geojson préalablement créés ou récupérés. Ces layers sont ajouté à la carte, les avalanches sont alors représentées sur la carte.<br>
                    Des scripts en JavaScript ou JQuery sont implémentés afin d’afficher les différents layers selon ce que souhaite l’utilisateur.
                </div>
            </div>
            <div class="row aboutUs">
                <h2>Le scrapper</h2>
                <div>
                    Toutes les parties collecte, formatage et analyse de données ont été réalisées en python.<br>
                    Le scrapper a, dans un premier temps, été développé avec la bibliothèque Scrapy avec l’IDE Spyder. <br>Or à la suite des conseils de notre entourage et dans un soucis de découverte, nous avons migré le projet sur PyCharm. <br>Par ailleurs, afin de pouvoir parcourir des sites web dont le contenu est géré dynamiquement, nous avons dû recourir aux solutions offertes par la lib. Selenium.<br>
                    Ce programme nous a permis de réunir 2035 avalanches supplémentaires à notre base de données.<br><br>
                    Le programme passe de page en page sur le site data-avalanche.org, et procède par étape :<br><br>
                    <ul id="scrapper" class="list-group">
                        <li>Il récupère la page HTML</li>
                        <li>Sélectionne la zone contenant les données nécessaires à la formation du dataset</li>
                        <li>Décode la chaine de caractère</li>
                        <li>Charge les données dans un dictionnaire</li>
                        <li>Ajoute les cases nécessaires à l’uniformisation de nos jeux de données</li>
                        <li>Retire les propriétés qui porte peu d’intérêt pour notre utilisation des données</li>
                        <li>Nettoie la propriété description de tout les caractère parasites (balise html, \n, \r, \t, etc…)</li>
                        <li>Ajoute la ligne à une liste cache</li>
                        <li>Transfert la liste dans un fichier json</li> 
                     </ul>


                </div>
            </div>  
            <div class="row aboutUs">
                <h2>L'organisation du travail</h2>
                <div>
                    Trois tâches ont rapidement été identifiées. <br>
                    La première étant de trouver des donnée, la seconde de créer une application web.<br><br>
                    La recherche de données a été réalisée en binôme, nous avons passé du temps à deux là-dessus.<br>
                    Par la suite l’un de nous s’est occupé principalement de l’application web en elle-même et du traitement des données récupérées afin de les afficher correctement sur la carte et de rendre l’application interactive.<br>
                    L’autre s’est chargé de créer un scrapper en python afin de parcourir le site data-avalanche.org, de récupérer les données concernant les avalanches cataloguées et de les traduire dans des fichiers geojson. <br>
                    Ces fichiers ont ensuite servi de base de données pour l’application web, en plus de celles déjà mise à disposition directement sur d’autre sites.<br><br>
                </div>
                <div>
                    Un dépôt GitHub a été créé, chacun pouvait donc avancer sur l’implémentation des fonctionnalités qu’il avait à réaliser.<br>
                    Au début du projet nous travaillions souvent ensemble à l’université ou chez nous. Tout le travail était réalisé en lors de ces séances fréquentes afin que chacun puisse participer à toutes les fonctionnalités.<br>
                    En revanche depuis le début du confinement, chacun les deux aspects du projet est devenu assez hermétiques et nous travaillions sur nos parties respectives, la communication étant devenue plus difficile. 
                </div>
            </div>  
            <div class="row aboutUs">
                <h2>Les défis</h2>
                <div>
                    Nous avons dû nous former sur de nouvelles technologies connues ou non pour mener à bien ce projet.<br><br>
                    Nous ne voulions pas faire un site web classique en tout html bien que cela aurait été possible et n’aurait rien changé au rendu, mais nous n’aurions pas progressé.<br>
                    Pour cela il a fallu se renseigner sur les Frameworks PHP existant et en sélectionner un adapté à la structure et l’envergure du projet.<br>
                    Plusieurs jours ont été nécessaires afin de nous familiariser avec Slim PHP et ses techniques.<br><br>
                    Au dernier moment nous avons découvert un conflit entre le serveur partagé hébergeant notre site et Slim PHP concernant les requêtes http.<br>
                    Une solution de secours tout html a été conçue en quelques dizaines de minutes. Finalement nous avons pu gérer ce problème et conserver une architecture plus avancée.<br><br>
                    La méthode d’affichage des données sur une carte interactive nous était totalement étrangère, et nos compétences en JavaScript et JQuery limitées.<br> 
                    Nous nous sommes donc formés pendant des jours sur ces technologies. <br><br>
                    L’outil leaflet proposé à été remplacé par mapbox qui est plus complet et possède une documentation très fournie ainsi qu’un système d’hébergement.<br>
                    Cette solution d’hébergement nous a été très utile concernant la partie photo-interprétation, <br>
                    les données étant trop volumineuses nous avons pu ainsi en déléguer le traitement ce qui a permis un gain de temps considérable au chargement de la carte.<br><br>
                    Enfin, comme cité précédemment, la crise sanitaire et le confinement que nous vivons a été un réel frein à l’avancement de notre projet. <br>
                    Collaborer à distance aura constitué un défi majeur que nous avons relevé en modifiant notre stratégie de fonctionnement.
                </div>
            </div>  
        </div>
        
        
        
    
    
    
    
     {% endblock %}